{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load();\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "    configuration: {\n",
    "        baseURL: \"https://blog.sayyou.icu/v1\",\n",
    "    },\n",
    "});\n",
    "//const model = new ChatOpenAI();\n",
    "\n",
    "await model.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI({\n",
    "    configuration: {\n",
    "        baseURL: \"https://blog.sayyou.icu/v1\",\n",
    "    },\n",
    "});\n",
    "//const chatModel = new ChatOpenAI();\n",
    "const outputPrase = new StringOutputParser();\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "await simpleChain.batch([\n",
    "    [ new HumanMessage(\"Tell me a joke\") ],\n",
    "    [ new HumanMessage(\"Hi, Who are you?\") ],\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "const stream = await simpleChain.stream([\n",
    "     new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    "    console.log(chunk)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "const stream = await simpleChain.streamLog([\n",
    "     new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    "    console.log(chunk)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T15:49:46.573859Z",
     "start_time": "2024-05-23T15:49:43.836440Z"
    }
   },
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "//const fakeLLM = new ChatOpenAI({\n",
    "//    azureOpenAIApiKey: \"123\",\n",
    "//    maxRetries: 0,\n",
    "//});\n",
    "\n",
    "//await fakeLLM.invoke(\"你好\")\n",
    "\n",
    "const realLLM = new ChatOpenAI({\n",
    "    configuration: {\n",
    "        baseURL: \"https://blog.sayyou.icu/v1\",\n",
    "    },\n",
    "});\n",
    "const llmWithFallback = fakeLLM.withFallbacks({\n",
    "    fallbacks: [realLLM]\n",
    "})\n",
    "\n",
    "await llmWithFallback.invoke(\"你好\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001B[33mtrue\u001B[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001B[32m\"你好！有什么可以帮助你的吗？\"\u001B[39m,\n",
       "    additional_kwargs: { function_call: \u001B[90mundefined\u001B[39m, tool_calls: \u001B[90mundefined\u001B[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001B[32m\"langchain_core\"\u001B[39m, \u001B[32m\"messages\"\u001B[39m ],\n",
       "  content: \u001B[32m\"你好！有什么可以帮助你的吗？\"\u001B[39m,\n",
       "  name: \u001B[90mundefined\u001B[39m,\n",
       "  additional_kwargs: { function_call: \u001B[90mundefined\u001B[39m, tool_calls: \u001B[90mundefined\u001B[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001B[33m7\u001B[39m, promptTokens: \u001B[33m9\u001B[39m, totalTokens: \u001B[33m16\u001B[39m },\n",
       "    finish_reason: \u001B[32m\"stop\"\u001B[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
